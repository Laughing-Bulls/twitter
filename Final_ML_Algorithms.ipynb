{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_ML_Algorithms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKWn386nqoht4+xouGc9k3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laughing-Bulls/twitter/blob/main/Final_ML_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to decide what machine learning algorithm we should implement for the sentiment analysis of tweets, let's go ahead and do some exploratory analysis:\n"
      ],
      "metadata": {
        "id": "kFH61-rDEviT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Set up:**"
      ],
      "metadata": {
        "id": "p_bw7RIJMTps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBMYhik5gVGH",
        "outputId": "e3388b16-e965-4023-f581-69127b71238d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 60.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=b23ad918314a11a13d593bda794fbc8abc87901faebf995b80fbeaeb1f23ffe1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ],
      "source": [
        "# Load the packages required\n",
        "\n",
        "!pip install pyspark\n",
        "\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.ml.feature import HashingTF\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark.sql.functions import split, regexp_replace\n",
        "from numpy import array\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "enzaJgzEOYux"
      },
      "outputs": [],
      "source": [
        "# Boilerplate Spark stuff:\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkDecisionTree\")\n",
        "sc = SparkContext(conf = conf)\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Load and prepare the necessary data:**"
      ],
      "metadata": {
        "id": "RavAasieMc_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RexllVOa0o",
        "outputId": "3843a32f-5731-49bf-b33f-3bce538bfb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data types:  [('_c0', 'int'), ('score', 'int'), ('words', 'string')]\n",
            "test data types:  [('_c0', 'int'), ('score', 'int'), ('words', 'string')] \n",
            "\n",
            "updated train data types:  [('_c0', 'int'), ('score', 'int'), ('words', 'array<string>')]\n",
            "updated test data types:  [('_c0', 'int'), ('score', 'int'), ('words', 'array<string>')] \n",
            "\n",
            "train overview: \n",
            "+---+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|_c0|score|words                                                                                                                                                      |\n",
            "+---+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0  |0    |['bummer',  'shoulda',  'got',  'david',  'carr',  'third',  'day',  'do',  'it',  'd']                                                                    |\n",
            "|1  |0    |['upset',  'that',  'cant',  'updat',  'hi',  'facebook',  'by',  'text',  'it',  'might',  'cri',  'as',  'result',  'school',  'today',  'also',  'blah']|\n",
            "|2  |0    |['dive',  'mani',  'time',  'ball',  'manag',  'save',  '',  'rest',  'go',  'out',  'bound']                                                              |\n",
            "|3  |0    |['whole',  'bodi',  'feel',  'itchi',  'like',  'it',  'on',  'fire']                                                                                      |\n",
            "|4  |0    |['no',  'it',  'not',  'behav',  'at',  'all',  'im',  'mad',  'whi',  'here',  'becaus',  'cant',  'see',  'all',  'over',  'there']                      |\n",
            "+---+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "test overview: \n",
            "+---+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|_c0|score|words                                                                                                                                                           |\n",
            "+---+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0  |4    |['loooooooovvvvvvee',  'kindl',  'not',  'that',  'dx',  'cool',  'but',  '',  'fantast',  'it',  'own',  'right']                                              |\n",
            "|1  |4    |['read',  'kindl',  'love',  'it',  'lee',  'child',  'good',  'read']                                                                                          |\n",
            "|2  |4    |['ok',  'first',  'asses',  'kindl',  'it',  'fuck',  'rock']                                                                                                   |\n",
            "|3  |4    |['youll',  'love',  'your',  'kindl',  'ive',  'mine',  'few',  'month',  'never',  'look',  'back',  'new',  'big',  'one',  'huge',  'no',  'need',  'remors']|\n",
            "|4  |4    |['fair',  'enough',  'but',  'have',  'kindl',  'think',  'it',  'perfect']                                                                                     |\n",
            "+---+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We read the processed data files\n",
        "# In order to read them like this we need to upload them to the \"Files\" of the Notebook\n",
        "train = spark.read.csv(\"processed_training_tweets.csv\", inferSchema=True, header=True)\n",
        "test = spark.read.csv(\"processed_test_tweets.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# We notice the issue that the \"words\" columns are type \"string\" instead of array<string> like we want\n",
        "print(\"train data types: \", train.dtypes)\n",
        "print(\"test data types: \", test.dtypes, \"\\n\")\n",
        "\n",
        "# We fix this issue and also change the type of the \"score\" column to float \n",
        "train = train.withColumn('words',split(regexp_replace(train[\"words\"], '\\[|\\]',''),',').cast('array<string>'))\n",
        "test = test.withColumn('words',split(regexp_replace(test[\"words\"], '\\[|\\]',''),',').cast('array<string>'))\n",
        "print(\"updated train data types: \", train.dtypes)\n",
        "print(\"updated test data types: \", test.dtypes, \"\\n\")\n",
        "\n",
        "# We remove the neutral tweets (score = 2) from the test data (we are going to be classifying tweets only as positive or negative)\n",
        "test = test[test[\"score\"] != 2]\n",
        "\n",
        "# Preview of the data\n",
        "print(\"train overview: \")\n",
        "train.show(truncate=False, n=5)\n",
        "print(\"test overview: \")\n",
        "test.show(truncate=False, n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dwtOXy_OfhA",
        "outputId": "41fa282d-e812-4b6e-e46e-1d95eb43479d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_train overview: \n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|score|words                                                                                                                                                      |numerical                                                                                                                                                                                       |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0    |['bummer',  'shoulda',  'got',  'david',  'carr',  'third',  'day',  'do',  'it',  'd']                                                                    |(262144,[46094,74842,81622,104610,110491,135781,141871,145792,165414,239626],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                         |\n",
            "|0    |['upset',  'that',  'cant',  'updat',  'hi',  'facebook',  'by',  'text',  'it',  'might',  'cri',  'as',  'result',  'school',  'today',  'also',  'blah']|(262144,[6281,32887,61269,71607,74842,78020,105225,106146,116317,119227,129769,145616,150886,217897,225715,228999,258408],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|0    |['dive',  'mani',  'time',  'ball',  'manag',  'save',  '',  'rest',  'go',  'out',  'bound']                                                              |(262144,[34466,53439,69420,71495,79469,101953,103563,159089,175771,201042,260323],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                |\n",
            "|0    |['whole',  'bodi',  'feel',  'itchi',  'like',  'it',  'on',  'fire']                                                                                      |(262144,[14374,27860,74842,90706,93683,112996,143483,258956],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                 |\n",
            "|0    |['no',  'it',  'not',  'behav',  'at',  'all',  'im',  'mad',  'whi',  'here',  'becaus',  'cant',  'see',  'all',  'over',  'there']                      |(262144,[6980,42090,44164,53941,71607,74842,87666,99911,140832,153671,217210,222730,243947,247908,258516],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                        |\n",
            "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "num_test overview: \n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|score|words                                                                                                                                                           |numerical                                                                                                                                                                                               |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|4    |['loooooooovvvvvvee',  'kindl',  'not',  'that',  'dx',  'cool',  'but',  '',  'fantast',  'it',  'own',  'right']                                              |(262144,[6980,22836,74842,79469,109516,116317,129242,178807,185204,202031,226111,239488],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                             |\n",
            "|4    |['read',  'kindl',  'love',  'it',  'lee',  'child',  'good',  'read']                                                                                          |(262144,[55161,66956,74842,108182,136613,139084,239488,261914],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                       |\n",
            "|4    |['ok',  'first',  'asses',  'kindl',  'it',  'fuck',  'rock']                                                                                                   |(262144,[46165,74842,138561,184262,219373,231411,239488],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                 |\n",
            "|4    |['youll',  'love',  'your',  'kindl',  'ive',  'mine',  'few',  'month',  'never',  'look',  'back',  'new',  'big',  'one',  'huge',  'no',  'need',  'remors']|(262144,[3486,24641,53431,60783,69417,74820,79415,91123,97863,103393,108359,142622,149656,183961,187506,223817,239488,261914],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|4    |['fair',  'enough',  'but',  'have',  'kindl',  'think',  'it',  'perfect']                                                                                     |(262144,[22836,74842,101612,182526,195543,198565,199811,239488],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                      |\n",
            "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We now transform the words to a numerical number and keep track of the count\n",
        "hashTF = HashingTF(inputCol=\"words\", outputCol=\"numerical\")\n",
        "num_train= hashTF.transform(train).select('score', 'words', 'numerical')\n",
        "num_test= hashTF.transform(test).select('score', 'words', 'numerical')\n",
        "\n",
        "# Preview of the modified data\n",
        "print(\"num_train overview: \")\n",
        "num_train.show(truncate=False, n=5)\n",
        "print(\"num_test overview: \")\n",
        "num_test.show(truncate=False, n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Train different models to find the best one:**"
      ],
      "metadata": {
        "id": "O2iTTYA3Mszp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qV3oXWI_Ohz6"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Training\n",
        "log_reg = LogisticRegression(labelCol = \"score\", featuresCol=\"numerical\", maxIter = 10, regParam = 0.01).fit(num_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gEO99XXOvIc",
        "outputId": "9b1d557d-4081-45f0-f3d1-17b36048a1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: \n",
            "\n",
            "results_log_reg overview: \n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "|words                                                                                                                                                           |prediction|score|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "|['loooooooovvvvvvee',  'kindl',  'not',  'that',  'dx',  'cool',  'but',  '',  'fantast',  'it',  'own',  'right']                                              |4.0       |4    |\n",
            "|['read',  'kindl',  'love',  'it',  'lee',  'child',  'good',  'read']                                                                                          |4.0       |4    |\n",
            "|['ok',  'first',  'asses',  'kindl',  'it',  'fuck',  'rock']                                                                                                   |4.0       |4    |\n",
            "|['youll',  'love',  'your',  'kindl',  'ive',  'mine',  'few',  'month',  'never',  'look',  'back',  'new',  'big',  'one',  'huge',  'no',  'need',  'remors']|0.0       |4    |\n",
            "|['fair',  'enough',  'but',  'have',  'kindl',  'think',  'it',  'perfect']                                                                                     |4.0       |4    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "# Correct predictions: 282 , # Data points: 359 , Accuracy: 0.7855153203342619\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression Prediction\n",
        "print(\"Logistic Regression: \\n\")\n",
        "pred_log_reg = log_reg.transform(num_test)\n",
        "results_log_reg = pred_log_reg.select(\"words\", \"prediction\", \"score\")\n",
        "print(\"results_log_reg overview: \")\n",
        "results_log_reg.show(truncate=False, n=5)\n",
        "\n",
        "correct_pred_log_reg = results_log_reg.filter(results_log_reg['prediction'] == results_log_reg['score']).count()\n",
        "print(\"# Correct predictions:\", correct_pred_log_reg, \", # Data points:\", results_log_reg.count(),\n",
        "      \", Accuracy:\", correct_pred_log_reg/results_log_reg.count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Training\n",
        "naive_bayes = NaiveBayes(labelCol = \"score\", featuresCol=\"numerical\", smoothing=1.0, modelType=\"multinomial\").fit(num_train)"
      ],
      "metadata": {
        "id": "Q7JDNKnBiXES"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Prediction\n",
        "print(\"Naive Bayes: \\n\")\n",
        "pred_naive_bayes = naive_bayes.transform(num_test)\n",
        "#accuracy_log_reg =  log_reg.transform(num_test).score\n",
        "results_naive_bayes = pred_naive_bayes.select(\"words\", \"prediction\", \"score\").replace(1.0, 4.0)\n",
        "print(\"results_naive_bayes overview: \")\n",
        "results_naive_bayes.show(truncate=False, n=5)\n",
        "\n",
        "correct_pred_naive_bayes = results_naive_bayes.filter(results_naive_bayes['prediction'] == results_naive_bayes['score']).count()\n",
        "print(\"# Correct predictions:\", correct_pred_naive_bayes, \", # Data points:\", results_naive_bayes.count(),\n",
        "      \", Accuracy:\", correct_pred_naive_bayes/results_naive_bayes.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0btqIVsi7wJ",
        "outputId": "fcf64499-998d-44f1-f7e8-205c5b2932ac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes: \n",
            "\n",
            "results_naive_bayes overview: \n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "|words                                                                                                                                                           |prediction|score|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "|['loooooooovvvvvvee',  'kindl',  'not',  'that',  'dx',  'cool',  'but',  '',  'fantast',  'it',  'own',  'right']                                              |4.0       |4    |\n",
            "|['read',  'kindl',  'love',  'it',  'lee',  'child',  'good',  'read']                                                                                          |4.0       |4    |\n",
            "|['ok',  'first',  'asses',  'kindl',  'it',  'fuck',  'rock']                                                                                                   |4.0       |4    |\n",
            "|['youll',  'love',  'your',  'kindl',  'ive',  'mine',  'few',  'month',  'never',  'look',  'back',  'new',  'big',  'one',  'huge',  'no',  'need',  'remors']|4.0       |4    |\n",
            "|['fair',  'enough',  'but',  'have',  'kindl',  'think',  'it',  'perfect']                                                                                     |4.0       |4    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "# Correct predictions: 304 , # Data points: 359 , Accuracy: 0.8467966573816156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Results:**\n",
        "\n",
        "We can see that, in this case, the Naive Bayes Prediction Model is the fastest to train. Its training was executed in 37 seconds compared to the almost 4 minutes that it took to train the Logistic Regression Model. Additionally, the Naive Bayes Prediction Model was also the most accurate, making correct predictions for 84.68% of the tweets. In comparison, the Logistic Regression Model made correct predictions 78.55% of the time. \n",
        "\n",
        "Therefore, the model that we are going to implement for the unsupervised sentiment analysis is going to be the Naive Bayes Model, since we were able to observe its efficancy in a supervised setting. "
      ],
      "metadata": {
        "id": "F-EBop9CNDXD"
      }
    }
  ]
}